---
layout: post
title: "Action chunking transformer — Notes"
tags: [ACT transformers, paper-notes]
---
The focus of this blog is ACT which stands for Action Chunking Transformer. But before that, let’s refresh our knowledge of transformers. Transformers is a type of neural network that can process and output sequences of information. A transformer is typically made up of three main layers which is the embedding layer, attention layer, and a mlp layer (where the attention and mlp layer are repeated multiple times). The transformer model is so unique because of its attention concept which, in short, allows the model to learn context from the sequences of input.

Now, when we think of transformers and tasks with “sequences of input”, we typically think of language translation, sentiment analysis, next word  prediction, etc. However, transformers can be applied to robotics as well in the form of ACT. In the discussion of ACT, I will specifically be talking about the ACT policy employed in the paper, “Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware” by Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn.

Though ACT can be applied to many robotics instances, for the rest of the blog, we will be focusing on a single robotic arm with a gripper with the task of picking up an object and dropping it into a box. 

Like with training any neural network model for a task, we need data. In this case, we set up cameras around the arm to capture videos of the arm picking up and dropping the object into a box while being teleoperated by a human. In addition to the video data, we also capture data relating to the joints/motors of the robot arm. There are other data captured such as velocity but for now let’s just focus on the video data and joint/motor data. To create our dataset, we record multiple instances of gripping the object and dropping into a box.

Now that we have our dataset, we need our ACT to pass the dataset into and train the model to be able to grab and drop the object into a box without teleoperation. Before we get into the specifics of the ACT model and what it looks like, we need to understand what ACT actually does. While a normal transformers model predicts a sequence of words, the ACT model predicts a sequence of actions in the form of robot joint/motor data. More specifically, given video data, the robot’s previous actions, and the robot’s current state, the model predicts the next sequence of actions required to complete the task. Another thing to note is that instead of predicting only the next single action at each timestep, the model predicts a short sequence or “chunk” of actions ahead of time. This is to enforce continuity. Real-world actions are often smooth so predicting multiple steps ahead ensures the actions are coherent. Predicting actions in chunks also reduces the number of forward passes through the model, saving computation.

Now that we know the purpose of ACT, let’s get into the architecture of the ACT model as outlined in the paper. Just a note, my discussion of the ACT model is simplified but captures the essence of how the ACT model generally works. 

There are three main layers in ACT which are the VAE Encoder (Variational Auto Encoder), the Main Encoder, and the Main Decoder where the VAE encoder is only enabled during training.  

The purpose of the VAE encoder is to introduce variation into the data and make sure the model doesn’t overfit which we will see in a bit. Although I won’t talk about it in detail, like in all transformers, the VAE encoder, as well as the main encoder and decoder all consist of attention layers and mlp layers and go through the process of gradient descent and backpropagation as discussed in the last blog. 

The input of the VAE encoder looks something like \[CLS, robot data, image data, past actions\] where image data refers to the features extracted from the video data by being fed into a convolutional neural network (CNN). Now, you may be wondering what CLS is. Well, CLS is known as the classification token. In other words, CLS is a single vector that summarizes the input data. CLS is initially a randomized vector and is learned by the model. This CLS token is then used to make a latent distribution. Now, what is a latent distribution? Well, when humans  record data, it isn’t perfect and there may be shaking. To make sure the robot doesn’t overfit and adapt imperfect human mistakes, we introduce variation to the data. To do this, we make a distribution. Imagine, a point cloud (a cluster of data points) of past actions and we randomly sample one point to use during training. This distribution or point cloud is what we call latent distribution. But how exactly do we create a latent distribution to sample from? Well, to make a distribution, we need mean and standard deviation. At the surface level, the mean and standard deviation is learned by the model with the use of weights and a loss function as usual. More specifically, for each input, the encoder produces two vectors, μ (mean) and log(σ²) (log variance) which are computed from the network outputs (e.g., the CLS token) via learned weights. Now that we have a latent distribution, we can sample a latent vector z. Think of vector z as a summary of past actions and other input with the addition of variation to prevent overfitting. 

Next is the main encoder. The input for the main encoder looks something like \[z, robot data, image data\]. In the main encoder, the input is passed through self-attention and MLP layers to produce contextualized representations of the current state

Finally, the input for the main decoder looks something like \[query tokens\]. The query tokens represent the future actions and is what the model needs to learn. In the main decoder, the input goes through self attention as well as cross attention (with regards to the output produced by the main encoder). Finally, the decoder outputs predicted sequences of future actions, which are compared to the ground truth via a loss function, and gradients are computed to update the weights of the VAE, main encoder, and main decoder through backpropagation.
